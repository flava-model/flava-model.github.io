<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<!-- ======================================================================= -->
<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>
<style type="text/css">
  body {
    font-family: "Titillium Web","HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    font-weight:300;
    font-size:18px;
    margin-left: auto;
    margin-right: auto;
    width: 100%;
  }

  h1 {
    font-weight:300;
    max-width: 100%;
  }

  div {
    max-width: 95%;
    margin:auto;
    padding: 10px;
  }

  .table-like {
    display: flex;
    flex-wrap: wrap;
    flex-flow: row wrap;
    justify-content: center;
  }

  .disclaimerbox {
    background-color: #eee;
    border: 1px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
    padding: 20px;
  }

  video.header-vid {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img {
    padding: 0;
    display: block;
    margin: 0 auto;
    max-height: 100%;
    max-width: 100%;
  }

  table {
    padding: 0;
    display: block;
    margin: 0 auto;
    max-height: 100%;
    max-width: 100%;
  }

  iframe {
    max-width: 100%;
  }

  img.header-img {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.rounded {
    border: 1px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  a:link,a:visited
  {
    color: #1367a7;
    text-decoration: none;
  }
  a:hover {
    color: #208799;
  }

  td.dl-link {
    height: 160px;
    text-align: center;
    font-size: 22px;
  }

  .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
            15px 15px 0 0px #fff, /* The fourth layer */
            15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
            20px 20px 0 0px #fff, /* The fifth layer */
            20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
            25px 25px 0 0px #fff, /* The fifth layer */
            25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
    margin-left: 10px;
    margin-right: 45px;
  }


  .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
    margin-top: 5px;
    margin-left: 10px;
    margin-right: 30px;
    margin-bottom: 5px;
  }

  .vert-cent {
    position: relative;
      top: 50%;
      transform: translateY(-50%);
  }

  hr
  {
    border: 0;
    height: 1px;
    max-width: 1100px;
    background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
  }

  #authors td {
    padding-bottom:5px;
    padding-top:30px;
  }
</style>
<!-- ======================================================================= -->

<!-- Start : Google Analytics Code -->
<!-- <script async src="https://www.googletagmanager.com/gtag/js?id=UA-64069893-4"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-64069893-4');
</script> -->
<!-- End : Google Analytics Code -->

<script type="text/javascript" src="resources/hidebib.js"></script>
<link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
<head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <link rel="apple-touch-icon" sizes="180x180" href="images/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="images/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="images/favicon-16x16.png">
  <link rel="manifest" href="images/site.webmanifest">
  <title>FLAVA: A Foundational Language And Vision Alignment Model</title>
  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <link rel="canonical" href="http://flava-model.github.io/" />
  <meta name="referrer" content="no-referrer-when-downgrade" />

  <meta property="og:site_name" content="FLAVA: A Foundational Language And Vision Alignment Model" />
  <meta property="og:type" content="video.other" />
  <meta property="og:title" content="FLAVA: A Foundation Language And Vision Alignment Model" />
  <meta property="og:description" content="Singh et. al. FLAVA: A Foundation Language And Vision Alignment Model. CVPR 2022." />
  <meta property="og:url" content="http://flava-model.github.io/" />
  <meta property="og:image" content="http://flava-model.github.io/images/method.png" />
  <!-- <meta property="og:video" content="https://www.youtube.com/embed/vQOHx8u_GWA?controls=0" /> -->

  <meta property="article:publisher" content="https://twitter.com/apsdehal" />
  <meta name="twitter:card" content="summary_large_image" />
  <meta name="twitter:title" content="FLAVA: A Foundation Language And Vision Alignment Model" />
  <meta name="twitter:description" content="Singh et. al. FLAVA: A Foundation Language And Vision Alignment Model. CVPR 2022." />
  <meta name="twitter:url" content="http://flava-model.github.io/" />
  <meta name="twitter:image" content="http://flava-model.github.io/images/method.png" />
  <meta name="twitter:site" content="@apsdehal" />
  <meta property="og:image:width" content="1600" />
  <meta property="og:image:height" content="900" />

  <script src="https://www.youtube.com/iframe_api"></script>
  <meta name="twitter:card" content="player" />
  <meta name="twitter:image" content="http://flava-model.github.io/images/method.png" />
  <!-- <meta name="twitter:player" content="https://www.youtube.com/embed/vQOHx8u_GWA?controls=0" /> -->
  <meta name="twitter:player:width" content="640" />
  <meta name="twitter:player:height" content="360" />
</head>

<body>
      <br>
      <center><span style="font-size:44px;font-weight:bold;">FLAVA: A Foundation Language And Vision Alignment Model</span></center><br/>
      <div class="table-like" style="justify-content:space-evenly;max-width:900px;margin:auto;">
        <div><center><span style="font-size:20px"><a href="https://apsdehal.in" target="_blank">Amanpreet Singh<sup>*</sup></a></span></center>
        </div>
        
        <div><center><span style="font-size:20px"><a href="https://ronghanghu.com/" target="_blank">Ronghang Hu<sup>*</sup></a></span></center>
        </div>
        
        <div><center><span style="font-size:20px"><a href="https://vedanuj.github.io/" target="_blank">Vedanuj Goswami<sup>*</sup></a></span></center>
        </div>
        
        <div><center><span style="font-size:20px"><a href="https://fr.linkedin.com/in/guillaume-couairon-35a814121" target="_blank">Guillaume Couairon</a></span></center>
        </div>
      </div>
        
      <div class="table-like" style="justify-content:space-evenly;max-width:600px;margin:auto;">
          <div><center><span style="font-size:20px"><a href="https://www.linkedin.com/in/galuba" target="_blank">Wojciech Galuba</a></span></center>
          </div>

          <div><center><span style="font-size:20px"><a href="https://rohrbach.vision/" target="_blank">Marcus Rohrbach</a></span></center>
          </div>

          <div><center><span style="font-size:20px"><a href="https://douwekiela.github.io/" target="_blank">Douwe Kiela</a></span></center>
          </div>
      </div>
      <table align=center width=600px style="padding-top:0px;padding-bottom:0px">
          <tr>
            <td align=center width=600px><center><span style="font-size:18px">Facebook AI Research</center></td>
          <tr/>
          <tr>
            <td align=center width=600px><center><span style="font-size:15px"><sup>*</sup> Equal Contribution</span></center></td>
          <tr/>
      </table>
      <center><span style="font-size:22px;"><a href=''>CVPR 2022</a></span></center><br/>

      <div class="table-like" style="justify-content:space-evenly;max-width:700px;margin:auto;padding:5px">
        <td align=center width=100px><center><span style="font-size:22px"><a href="https://arxiv.org/abs/2112.04482">[Paper]</a></span></center></td>
        <td align=center width=100px><center><span style="font-size:22px"><a href="https://github.com/facebookresearch/multimodal/tree/main/examples/flava">[Code]</a></span></center></td>
        <td align=center width=100px><center><span style="font-size:22px"><a href="https://huggingface.co/facebook/flava-full">[Model]</a></span></center></td>
        <td align=center width=100px><center><span style="font-size:22px"><a href="https://huggingface.co/datasets/facebook/pmd">[Data]</a></span></center></td>
      </div><br/>

      <center>
      <!-- <iframe width="768" height="432" max-width="100%" src="https://www.youtube.com/embed/vQOHx8u_GWA?controls=0" frameborder="3" allowfullscreen></iframe> -->
      <img max-width="100%" src="images/method.png" frameborder="3" allowfullscreen></iframe>
      </center>
      <br>

      <div style="width:800px; margin:0 auto; text-align: left;">
        State-of-the-art vision and vision-and-language models rely on large-scale visio-linguistic pretraining for obtaining
        good performance on a variety of downstream tasks. Generally, such models are often either cross-modal (contrastive) or
        multi-modal (with earlier fusion) but not both; and they often only target specific modalities or tasks. A promising
        direction would be to use a single holistic universal model, as a "foundation", that targets all modalities at once -- a
        true vision and language foundation model should be good at vision tasks, language tasks, and cross- and multi-modal
        vision and language tasks. We introduce FLAVA as such a model and demonstrate impressive performance on a wide range of
        35 tasks spanning these target modalities.
      </div>
      <br><hr>

      <center><h1>FLAVA Model</h1></center>
      <div style="width:800px; margin:0 auto; text-align: left">
        FLAVA, a language and vision alignment model
        learns strong representations from multimodal (imagetext pairs) and unimodal data (unpaired images and text)
        and can
        be applied to target a broad scope of tasks from three domains
        (visual recognition, language understanding, and multimodal reasoning) under a common transformer model architecture.
      </div>
        <center><a href="images/teaser.png"><img src = "images/teaser.png" height="200px"></img></a><br></center>
      <hr>


      <!-- <center><h1>Overview of the Algorithm</h1></center><br/>
      <center><a href="resources/method.png"><img src = "resources/method.png" width="800px"></img></a><br></center>
      <br/><hr> -->


            <center id="sourceCode"><h1>Source Code</h1></center>
            <div style="width:800px; margin:0 auto; text-align: center">
            Source code is available at <a href="https://github.com/facebookresearch/multimodal/tree/main/examples/flava">this link</a> 
              and pretrained models can also be find on <a href="https://huggingface.co/facebook/flava-full">Hugging Face</a>. Follow <a href="https://twitter.com/apsdehal">Amanpreet</a> on Twitter for latest updates.
            </div>
            <table align=center width=600px>
              <tr>
              </tr>
            </table>
            <br><hr>

              <center><h1>Paper and Bibtex</h1></center>
              <table align=center width=850px>
              <tr style="width:800px; margin:0 auto; text-align: right">
              <!-- <td style="width:800px; margin:0 auto; text-align: right"> -->
                <a style="width:800px; margin:0 auto; text-align: right" href="https://arxiv.org/abs/2112.04482">
                  <img style="width:800px; margin:0 auto; text-align: right" src="resources/thumbnail.jpeg"/>
                </a>
                <center>
                <span style="font-size:20pt"><a href="https://arxiv.org/pdf/2112.04482.pdf">[Paper]</a>
                <span style="font-size:20pt"><a href="https://arxiv.org/abs/2112.04482">[ArXiv]</a>
                <span style="font-size:20pt"><a shape="rect" href="javascript:togglebib('plan2explore2019_bib')" class="togglebib">[Bibtex]</a></span>

                </center>
              <!-- </td> -->
              <!-- <td width="6%" align=center>
              </td> -->
              </tr>
              <tr>                
                              <td width="64%" align=left>
                                <div class="paper" id="plan2explore2019_bib">
<pre xml:space="preserve">
@inproceedings{singh2022flava,
  author = {
    Amanpreet Singh and
    Ronghang Hu and
    Vedanuj Goswami and
    Guillaume Couairon and
    Wojciech Galuba and
    Marcus Rohrbach and
    Douwe Kiela
  },
  title = {
    {FLAVA:} {A} Foundational Language And 
    Vision Alignment Model
  },
  booktitle={CVPR},
  year={2022}
}
</pre>
                                </div>
                              </td>

              </tr>
            </table>
          <br><hr>
      </table>

<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>
</div>
<table align=center width=800px>
  <tr><td width=800px><left>
  <center><h1>Acknowledgements</h1></center>
  We thank Devi Parikh for her support and advice on this
  project. We are grateful to Dmytro Okhonko, Hu Xu, Armen Aghajanyan, Po-Yao Huang, Min Xu, and Aleksandra Piktus for
  joint explorations of multimodal data. We
  thank Ning Zhang, Madian Khabsa, Sasha Sheng, and Naman Goyal for useful technical discussions; Karan Desai
  for providing access to RedCaps; Vaibhav Singh and others on the Google TPU team for help running experiments
  on TPUs; Shubho Sengupta, Armand Joulin, Brian O’Horo,
  Arthur Menezes for compute and storage support; and Ryan
  Jiang, Kushal Tirumala and Russ Howes for help running
  experiments.
</left></td></tr>
</table>
<br><br>
</body>
</html>
